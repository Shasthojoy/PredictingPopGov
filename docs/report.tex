\documentclass[11pt]{article}
\setlength{\oddsidemargin}{0pt}
\usepackage{amsfonts}
\usepackage{latexsym}
\usepackage{listings}

\title{Predicting Popular Government, an analysis of states}

\author{Justin Hines\\
{\tt jph2149@columbia.edu}
}

\begin{document}

\maketile

\section{Abstract}

The term popularity in the modern of era takes on new meaning, especially 
considering when the exchange of information is instantaneous.  Information is
now a commodity with a viral life. The virality of 
information the internet can change public sentiment towards a variety of 
topics, products, andn issues.  In this fold has come the explotation of 
information in attempts to increase its virality. The commidification
of information by content producers has led to the expotation of said 
information, at the cost of clarity, correctness, and quality, all to attract
more end-users.  Popularity now is a means of sucess based on the merits of 
"likes", upvotes, retweets, or clicks.  \\
\\
The demand of our increasingly digital world and the instantaneous form of 
information has forced our government to enter this realm and become a content 
provider.  As such, the federal government's public image is subject to the 
virality of information. As a result, the government naturally tries to better
its public image by catering content towards its end users.  NASA is now 
popular, thanks to death, but also the virality of information spread on social
networks and the great content being pushed by NASA. \\
\\
But at what cost does this commodification occur, especially within the realm 
of government? Is it possible to garner some statisically meaning of the words 
in this content and how it relates to popularity.  If so, it would seem 
reasonable that if such a satistical trend exists, we could exploit to predict
what articles will be popular and how popular they will be.\\
\\
Thus, the goal of our modeling will be to classify articles into popularity 
levls using a Bernoulli Naive Bayes Classifer.  Such a model will be scalable
for the large dataset described below, but should also provide some insight
the word choice popularity distrubted amongst the documents, which should
prove interesting.
\section {Data}

The initial dataset for this project came from an achieved data set provided by
usa.gov in conjunction with Bit.ly.  The dataset, which can be found at 
http://bitly.measuredvoice.com/bitly_archive/?C=M;O=D and additional 
information at http://www.usa.gov/About/developer-resources/1usagov.shtml#data,
consists of a variety of data points about end-user clicks on a shorted 
governmental link.  Most governmental websites publish information through 
these links as a means of collecting analytics. The data is stored as a pub/sub
stream of JSON entries, one per line, that represent real-time clicks when the 
data is collected.  In the achieved data, the data are stored similarly.  
An entry consists of a JSON dictionary with a variety of different data points, 
including with interest to us, a bitly global hash of link, the end link, and 
the location where that click came from.\\ 
\\
The dataset includes entires from December 2011 to present day.  For the sake 
of sanity a subset of the data was used, from January 1st, 2012 to April 30th, 
2012.\\ 
\\
While this collection is certainly interesting, it provides no real insights on 
how information is being shared by what states, and what information is popular 
where, and why.  In addition, this information provides no analysis on the end 
sight beyond its link.\\
\\
In order to clean the dataset, a script was written that would filter out an 
clicks not occuring the United States (this was defined if the data entity 
contained an originating information, if that location was not in one of the 
fify United States).  In addition, in order to collect more information about 
the end site, the site at the link destination was downloaded, and filtered for
content only contained in the title or in paragraph tags.  Any sites that 
did not have over fifty words between the title and in paragraphs was 
automatically excluded.  Also, a counter was kept for each time a link was seen being 
clicked by an individual state, in addition to a global counter that kept track 
of everytime a link was clicked, regardless of the state it orginated from.\\ 
\\
In order to compute the above, since the dataset is rather large with clicks 
being on the order of mangitiate of 600k with 13k unique articles, the work
was divided and computed on an ec2 cluster, with each month being divied into 6
parts. \\
\\
In order to apply some meaning on the popularity of content (a strict threshold 
count was excluded as referring state populations differ dramaticaly), links 
were sorted by popularity in terms of click frequency, and then divided into 5
equal labels, in order from least to greatest to highest, low, medium, high, 
popular, and highly popular. The author recongizes this a rather 
naive method of applying labels, as an article in highly popular and high may 
differ by a non-statsically significant amount, but rather both lie on a 
boundary.  For sanity however, this method was applied.\\
\\
The resulting data was stored in tabular separatred value files, grouped by 
state, along with a global, with the first entry containing the global hash, location, number of 
clicks, and link content.\\ 
\\
To run the data collection and refinement system
\section{System}
For simplicity, a Bernoulli model of word occurrence in documents was used to 
generate a Naive Bayes Classification of labels, where each document is moduled 
as vector with the probablity, or weights, based on the probability of each 
word occuring in a document independently.  A Naive Bayes classifer was choosen 
for its relative cheap computation and scalabity over other models for both 
training and predictions, as training is a simple linear count of word 
frequency and prediction. Similarly, predcitions in a trained model, calculating
the logs odds is linear.\\
\\
Training and log-odds predictions were very similar to those presented in here
http://jakehofman.com/ddm/wp-content/uploads/2012/03/homework_02.pdf
For the sake of not being repeatative, I leave it to the reader to read the 
article for a further information on the classifiers.\\
\\
Performace was evaluated grouped by state on a series of different \alpha and 
\beta for smoothing.  For each state a 5x5 matrix was calculated along with 
10 most informative words for each label. Each of these runs is stored in
\begin{verbatim} \log\{STATE}.log \end{verbatim} 
\section{Analysis}
Unfortunately, the reduced dataset and filtering showed for many states to not 
have very many results, and thus our feature space is rather small which made
classification rather difficult for these states.  However our global set 
had a fairly rich feature set and provided to be more interesting.

\end{document}
